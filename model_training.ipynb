{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T14:29:52.537973Z",
     "iopub.status.busy": "2025-09-16T14:29:52.537772Z",
     "iopub.status.idle": "2025-09-16T14:29:52.544607Z",
     "shell.execute_reply": "2025-09-16T14:29:52.544101Z",
     "shell.execute_reply.started": "2025-09-16T14:29:52.537956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:29:52.545855Z",
     "iopub.status.busy": "2025-09-16T14:29:52.545413Z",
     "iopub.status.idle": "2025-09-16T14:30:34.305510Z",
     "shell.execute_reply": "2025-09-16T14:30:34.304821Z",
     "shell.execute_reply.started": "2025-09-16T14:29:52.545731Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:30:16.151603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758033016.500315      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758033016.595721      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n",
      "Environment setup completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    TaskType, \n",
    "    PeftConfig,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "# Environment configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "print(\"Environment setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:34.307669Z",
     "iopub.status.busy": "2025-09-16T14:30:34.307101Z",
     "iopub.status.idle": "2025-09-16T14:30:34.699751Z",
     "shell.execute_reply": "2025-09-16T14:30:34.698971Z",
     "shell.execute_reply.started": "2025-09-16T14:30:34.307649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /kaggle/input/chat-data/combined1.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fe59835c77490788feecb177172082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 833 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0428b9396114549bc29903764001d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset converted to instruction format\n",
      "Sample example:\n",
      "Input: Extract action items as JSON from this dialogue: Olivia: I just got an offer on LinkedIn\n",
      "Anne: Where from?\n",
      "Olivia: France, Project Manager\n",
      "Mike: Ok, b...\n",
      "Target: [\n",
      "  {\n",
      "    \"task\": \"Research the job offer from the creative agency in Rennes, France\",\n",
      "    \"owner\": \"Olivia\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Discuss the pros and cons of moving to France with Olivia\",\n",
      "    \"owner\": \"Group\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Decide on Olivia's interest in the job offer\",\n",
      "    \"owner\": \"Olivia\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Let the creative agency know Olivia's interest in the job offer\",\n",
      "    \"owner\": \"Olivia\",\n",
      "    \"due_date\": \"None\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load action item dataset with robust error handling\n",
    "DATASET_PATH = \"/kaggle/input/chat-data/combined1.jsonl\"\n",
    "\n",
    "def load_action_item_dataset():\n",
    "    \"\"\"Load dataset with fallback options\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading dataset from: {DATASET_PATH}\")\n",
    "        raw_dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
    "        print(f\"Dataset loaded successfully with {len(raw_dataset)} samples\")\n",
    "        \n",
    "        # Convert to instruction format\n",
    "        def convert_format(example):\n",
    "            return {\n",
    "                \"input_text\": f\"Extract action items as JSON from this dialogue: {example['dialogue']}\",\n",
    "                \"target_text\": example[\"actions\"]\n",
    "            }\n",
    "        \n",
    "        dataset = raw_dataset.map(convert_format)\n",
    "        print(\"Dataset converted to instruction format\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"Sample example:\")\n",
    "        print(f\"Input: {dataset[0]['input_text'][:150]}...\")\n",
    "        print(f\"Target: {dataset[0]['target_text']}\")\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading main dataset: {e}\")\n",
    "        print(\"Creating a small fallback dataset to allow the notebook to run end-to-end.\")\n",
    "        # Tiny synthetic fallback data (repeated) so training can proceed if real data is missing\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"input_text\": \"Extract action items as JSON from this dialogue:\\nAlice: We need to finish the report by Thursday.\\nBob: I will review it tomorrow.\\nAlice: Please ping the team.\",\n",
    "                \"target_text\": '[{\"task\": \"Finish report\", \"owner\": \"Alice\", \"due_date\": \"Thursday\"}, {\"task\": \"Review report\", \"owner\": \"Bob\", \"due_date\": \"tomorrow\"}, {\"task\": \"Ping the team\", \"owner\": \"Alice\", \"due_date\": \"None\"}]'\n",
    "            },\n",
    "            {\n",
    "                \"input_text\": \"Extract action items as JSON from this dialogue:\\nJohn: Can you book a meeting room for Friday?\\nSara: Sure, I will also prepare slides.\\nJohn: Great, send the calendar invite.\",\n",
    "                \"target_text\": '[{\"task\": \"Book meeting room\", \"owner\": \"Sara\", \"due_date\": \"Friday\"}, {\"task\": \"Prepare slides\", \"owner\": \"Sara\", \"due_date\": \"None\"}, {\"task\": \"Send calendar invite\", \"owner\": \"John\", \"due_date\": \"None\"}]'\n",
    "            },\n",
    "            {\n",
    "                \"input_text\": \"Extract action items as JSON from this dialogue:\\nTeam: We should deploy on Wednesday.\\nPM: David, please handle the rollout plan.\\nDavid: Will do.\\nEmily: I\\'ll prepare the agenda.\",\n",
    "                \"target_text\": '[{\"task\": \"Deploy\", \"owner\": \"Team\", \"due_date\": \"Wednesday\"}, {\"task\": \"Rollout plan\", \"owner\": \"David\", \"due_date\": \"None\"}, {\"task\": \"Prepare agenda\", \"owner\": \"Emily\", \"due_date\": \"None\"}]'\n",
    "            }\n",
    "        ] * 150\n",
    "        \n",
    "        dataset = Dataset.from_list(sample_data)\n",
    "        print(f\"Created fallback dataset with {len(dataset)} examples\")\n",
    "        return dataset\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_action_item_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:34.701122Z",
     "iopub.status.busy": "2025-09-16T14:30:34.700809Z",
     "iopub.status.idle": "2025-09-16T14:30:41.211650Z",
     "shell.execute_reply": "2025-09-16T14:30:41.210967Z",
     "shell.execute_reply.started": "2025-09-16T14:30:34.701095Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FLAN-T5 model: google/flan-t5-base\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facb3b42288b47638c4a85b1cec0790e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055ac98a51314b07a0634a614b9478a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9d005fa63a4e78bb9b28b44b4ebcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da8fd146b5b4017b88d9a11fd5444a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocab size: 32100\n",
      "Error loading model: No package metadata was found for bitsandbytes\n",
      "Attempting fallback model loading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bcdfab5824456397ed577a64932953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1429cdf0574f43dda1d504c3ab1917d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637053c91e0d4abbaf2c3ec7d692dfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "def load_flan_t5_model():\n",
    "    \"\"\"Load FLAN-T5 with proper error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading FLAN-T5 model: {MODEL_NAME}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")\n",
    "        \n",
    "        # Quantization config for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\", \n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            print(\"Using 4-bit quantization for GPU\")\n",
    "        else:\n",
    "            quantization_config = None\n",
    "            print(\"Using full precision for CPU\")\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        )\n",
    "        \n",
    "        # Prepare for quantized training if using GPU\n",
    "        if quantization_config is not None:\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        print(f\"FLAN-T5 model loaded with {model.num_parameters():,} parameters\")\n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        # Fallback to basic loading\n",
    "        print(\"Attempting fallback model loading\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "        return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_flan_t5_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETER-1 (PERFORMED BETTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:41.212626Z",
     "iopub.status.busy": "2025-09-16T14:30:41.212425Z",
     "iopub.status.idle": "2025-09-16T14:30:41.317520Z",
     "shell.execute_reply": "2025-09-16T14:30:41.316688Z",
     "shell.execute_reply.started": "2025-09-16T14:30:41.212609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n",
      "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n",
      "LoRA applied successfully\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for FLAN-T5\n",
    "def setup_lora(model):\n",
    "    \"\"\"Setup LoRA configuration with error handling\"\"\"\n",
    "    try:\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q\", \"v\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "            inference_mode=False\n",
    "        )\n",
    "        \n",
    "        print(\"Applying LoRA configuration...\")\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Display trainable parameters\n",
    "        model.print_trainable_parameters()\n",
    "        print(\"LoRA applied successfully\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error applying LoRA: {e}\")\n",
    "        print(\"Training will continue without LoRA (full fine-tuning)\")\n",
    "        return model\n",
    "\n",
    "# Apply LoRA\n",
    "model = setup_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:41.318780Z",
     "iopub.status.busy": "2025-09-16T14:30:41.318477Z",
     "iopub.status.idle": "2025-09-16T14:30:41.332283Z",
     "shell.execute_reply": "2025-09-16T14:30:41.331571Z",
     "shell.execute_reply.started": "2025-09-16T14:30:41.318753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/test split...\n",
      "Train size: 666 | Test size: 167\n"
     ]
    }
   ],
   "source": [
    "# --- Train/Test Split (added) ---\n",
    "from datasets import DatasetDict\n",
    "\n",
    "print(\"Creating train/test split...\")\n",
    "# 20% test split with fixed seed for reproducibility\n",
    "splits = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = splits[\"train\"]\n",
    "test_dataset  = splits[\"test\"]\n",
    "print(f\"Train size: {len(train_dataset)} | Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:41.333107Z",
     "iopub.status.busy": "2025-09-16T14:30:41.332905Z",
     "iopub.status.idle": "2025-09-16T14:30:42.767255Z",
     "shell.execute_reply": "2025-09-16T14:30:42.766404Z",
     "shell.execute_reply.started": "2025-09-16T14:30:41.333082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873283431dd74c268b2e4aa84d42a84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessing completed: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 833\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332ad770c5c5426297f1ec7b25519136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ab3d650d9f425d8b364b47f97cc77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 666\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 167\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing function\n",
    "def preprocess_dataset(examples):\n",
    "    \"\"\"Preprocess dataset for FLAN-T5 training\"\"\"\n",
    "    inputs = examples[\"input_text\"]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    targets = examples[\"target_text\"]  \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "try:\n",
    "    tokenized_dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=dataset.column_names)\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset preprocessing completed: {tokenized_dataset}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Preprocessing error: {e}\")\n",
    "    # Fallback: simpler preprocessing\n",
    "    print(\"Using fallback preprocessing\")\n",
    "    tokenized_dataset = dataset\n",
    "    data_collator = None\n",
    "\n",
    "# --- Preprocess train & test splits (added) ---\n",
    "try:\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_dataset, batched=True, remove_columns=train_dataset.column_names)\n",
    "    tokenized_test_dataset  = test_dataset.map(preprocess_dataset,  batched=True, remove_columns=test_dataset.column_names)\n",
    "    \n",
    "    # Keep original variable name for training compatibility\n",
    "    tokenized_dataset = tokenized_train_dataset\n",
    "    \n",
    "    print(\"Tokenization complete.\")\n",
    "    print(tokenized_train_dataset)\n",
    "    print(tokenized_test_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Split preprocessing error: {e}\")\n",
    "    # Fallback: use raw splits if mapping fails\n",
    "    tokenized_train_dataset = train_dataset\n",
    "    tokenized_test_dataset  = test_dataset\n",
    "    tokenized_dataset = tokenized_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:30:42.769054Z",
     "iopub.status.busy": "2025-09-16T14:30:42.768455Z",
     "iopub.status.idle": "2025-09-16T14:32:31.824242Z",
     "shell.execute_reply": "2025-09-16T14:32:31.823112Z",
     "shell.execute_reply.started": "2025-09-16T14:30:42.769032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.36.0\n",
      "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate==0.24.1\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets==2.14.0\n",
      "  Downloading datasets-2.14.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft==0.6.2\n",
      "  Downloading peft-0.6.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting safetensors==0.4.1\n",
      "  Downloading safetensors-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (2.32.4)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.36.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.24.1) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.24.1) (2.6.0+cu124)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.0) (19.0.1)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.0)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.14.0) (2025.5.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.0) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.36.0) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.36.0) (2025.6.15)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate==0.24.1)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.24.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.24.1) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.14.0)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.0) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.24.1) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.36.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.36.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.36.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.36.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.36.0) (2024.2.0)\n",
      "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.14.0-py3-none-any.whl (492 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, peft, datasets\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.8.1\n",
      "    Uninstalling accelerate-1.8.1:\n",
      "      Successfully uninstalled accelerate-1.8.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.15.2\n",
      "    Uninstalling peft-0.15.2:\n",
      "      Successfully uninstalled peft-0.15.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.6.0\n",
      "    Uninstalling datasets-3.6.0:\n",
      "      Successfully uninstalled datasets-3.6.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.24.1 datasets-2.14.0 dill-0.3.7 multiprocess-0.70.15 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.6.2 safetensors-0.4.1 tokenizers-0.15.2 transformers-4.36.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers==4.36.0\" \"accelerate==0.24.1\" \"datasets==2.14.0\" \"peft==0.6.2\" \"safetensors==0.4.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:32:31.827764Z",
     "iopub.status.busy": "2025-09-16T14:32:31.827482Z",
     "iopub.status.idle": "2025-09-16T14:32:32.078408Z",
     "shell.execute_reply": "2025-09-16T14:32:32.077717Z",
     "shell.execute_reply.started": "2025-09-16T14:32:31.827740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration completed\n",
      "Batch size: 4\n",
      "Learning rate: 0.0002\n",
      "Epochs: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrainingArguments, IntervalStrategy\n",
    "OUTPUT_DIR = \"/kaggle/working/flan-t5-action-extractor\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    # Nudge: 1e-3 is hot for T5+LoRA; start safer.\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=10,\n",
    "    save_strategy=IntervalStrategy.EPOCH,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[],                              # more robust than None on some versions\n",
    "    load_best_model_at_end=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=False,\n",
    "    bf16=torch.cuda.is_available() and hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported(),\n",
    "    fp16=torch.cuda.is_available() and not (hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported()),\n",
    ")\n",
    "print(\"Training configuration completed\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:32:32.079376Z",
     "iopub.status.busy": "2025-09-16T14:32:32.079115Z",
     "iopub.status.idle": "2025-09-16T14:35:45.011952Z",
     "shell.execute_reply": "2025-09-16T14:35:45.010980Z",
     "shell.execute_reply.started": "2025-09-16T14:32:32.079358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FLAN-T5 training...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [252/252 03:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.880900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training completed successfully\n",
      "Final training loss: 1.0701\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer and start training\n",
    "def run_training():\n",
    "    \"\"\"Execute training with error handling\"\"\"\n",
    "    try:\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,     # <-- uses train split via our reassignment\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting FLAN-T5 training...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Start training\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        print(\"Training completed successfully\")\n",
    "        print(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
    "        \n",
    "        return trainer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training error: {e}\")\n",
    "        print(\"Training failed - check memory, batch size, or model configuration\")\n",
    "        return None\n",
    "\n",
    "# Execute training\n",
    "trainer = run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:45.013922Z",
     "iopub.status.busy": "2025-09-16T14:35:45.013292Z",
     "iopub.status.idle": "2025-09-16T14:40:09.550542Z",
     "shell.execute_reply": "2025-09-16T14:40:09.549758Z",
     "shell.execute_reply.started": "2025-09-16T14:35:45.013893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe4867659fc472cbf7154f2fabd1045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Metrics ---\n",
      "test_size: 167\n",
      "exact_match: 0.0000\n",
      "rougeL_f1: 0.4012\n",
      "bow_f1: 0.4112\n",
      "avg_pred_len: 18.8862\n",
      "avg_ref_len: 28.0599\n",
      "\n",
      "--- Sample Predictions ---\n",
      "Example 1\n",
      "INPUT: Extract action items as JSON from this dialogue: Katy: I'm at the entrance Katy: where are you? Lucy: as well, lol Lily: I'm with Lucy Jorge: I'm still in the library Katy: but what entrance? of the library? Lucy: no! the main entrance to the university Katy: ok, i'll be there in 5 min\n",
      "REF:   [\n",
      "  {\n",
      "    \"task\": \"Meet at the main entrance to the university\",\n",
      "    \"owner\": \"Katy\",\n",
      "    \"due_date\": \"in 5 min\"\n",
      "  }\n",
      "]\n",
      "PRED:  [  \"task\": \"Be at the entrance of the library\", \"owner\": \"Katy\", \"due_date\": \"5 min\"  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Example 2\n",
      "INPUT: Extract action items as JSON from this dialogue: Chloe: So guess what I heard today! Patrick: Omg girl what?? Chloe: I GOT INTO GRAD SCHOOL!  Patrick: I'M SO PROUD OF YOU!  Chloe: Thank you! We have to go out tonight! I was thinking the Abbey? Patrick: Perfect. I'll tell my roommates. Say around 10 \n",
      "REF:   [\n",
      "  {\n",
      "    \"task\": \"Tell roommates about going out to the Abbey\",\n",
      "    \"owner\": \"Patrick\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Meet Chloe at the Abbey at 10 pm\",\n",
      "    \"owner\": \"Patrick\",\n",
      "    \"due_date\": \"None\"\n",
      "  }\n",
      "]\n",
      "PRED:  [  \"task\": \"Go out tonight at the Abbey\", \"owner\": \"Patrick\", \"due_date\": \"around 10 pm\"  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Example 3\n",
      "INPUT: Extract action items as JSON from this dialogue: Billy: cute profile pic Melanie: thank you Billy: is this your cat? Melanie: unfortunately not :( Melanie: my landlord doesn't let me have a pet Billy: that's crazy that landlords are more strict about pets than about kids Melanie: exactly! Billy: do \n",
      "REF:   [\n",
      "  {\n",
      "    \"task\": \"Show Melanie a photo of Billy's cat\",\n",
      "    \"owner\": \"Billy\",\n",
      "    \"due_date\": \"None\"\n",
      "  }\n",
      "]\n",
      "PRED:  [  \"task\": \"Recently see the profile picture of Melanie\", \"owner\": \"Billy\", \"due_date\": \"None\"  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Example 4\n",
      "INPUT: Extract action items as JSON from this dialogue: Fai: Tomorrow i'm going to the skatepark. Larry: What time? Maybe we can make some videos for the clip, the day will be sunny. Fai: Around 10:00 AM. Gonna stay there until lunch time. Larry: Nice, I will meet you there. Don't forget to use the sponsor\n",
      "REF:   [{\"task\": \"Meet Fai at the skatepark\", \"owner\": \"Larry\", \"due_date\": \"2025-09-14\"}, {\"task\": \"Use sponsor's clothes for video shoot\", \"owner\": \"Fai\", \"due_date\": \"2025-09-14\"}]\n",
      "PRED:  [  \"task\": \"Make videos for the clip\", \"owner\": \"Fai\", \"due_date\": \"None\"  ]\n",
      "--------------------------------------------------------------------------------\n",
      "Example 5\n",
      "INPUT: Extract action items as JSON from this dialogue: Charlotte: I just checked and I'm gonna arrive at 22:25. I can't believe I didn't check it well ... Oscar: Hahahhahaa. So what will you do now? Charlotte: Soooo, I was thinking that if she tells you that my aunt cancelled the plans, you can tell her t\n",
      "REF:   [\n",
      "  {\n",
      "    \"task\": \"Tell her that you can see a movie together tomorrow night\",\n",
      "    \"owner\": \"Oscar\",\n",
      "    \"due_date\": \"tomorrow night\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Make another dinner tomorrow with her\",\n",
      "    \"owner\": \"Oscar\",\n",
      "    \"due_date\": \"tomorrow\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Arrange a big box to go inside\",\n",
      "    \"owner\": \"Charlotte\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Open the box for Charlotte when she arrives home\",\n",
      "    \"owner\": \"Oscar\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Find a movie on Netflix that her mum will like\",\n",
      "    \"owner\": \"Charlotte\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Tell her mum that Oscar knows magic and make it funny\",\n",
      "    \"owner\": \"Oscar\",\n",
      "    \"due_date\": \"tomorrow\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Text Oscar when she arrives home and is ready to get in\",\n",
      "    \"owner\": \"Charlotte\",\n",
      "    \"due_date\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Take her mum to Oscar's room so Charlotte has more time\",\n",
      "    \"owner\": \"Oscar\",\n",
      "    \"due_date\": \"tomorrow\"\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Let Oscar know when she lands and when she gets to the building\",\n",
      "    \"owner\": \"Charlotte\",\n",
      "    \"due_date\": \"None\"\n",
      "  }\n",
      "]\n",
      "PRED:  [  \"task\": \"Agree to see a movie with Charlotte\", \"owner\": \"Oscar\", \"due_date\": \"None\" ,  \"task\": \"Agree to arrange a big box to go inside the flat\", \"owner\": \"Oscar\", \"due_date\": \"None\" ,  \"task\": \"Agree to arrange a big box to go inside the flat\", \"owner\": \"Oscar\", \"due_date\": \"None\" ,  \"task\": \"Agree to arrange a big box to go inside the flat\", \"owner\": \"Oscar\", \"due_date\": \"None\" ,  \"task\": \"Agree to arrange a big box to go inside the flat\", \"owner\": \"Oscar\", \"due_date\": \"None\" ,  \"task\": \"Agree to arrange a big box to go inside the flat\", \"owner\": \"Oscar\", \"due\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluation on held-out test set (added) ---\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import isfinite\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def _normalize_text(s: str):\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "def _lcs_len(x_tokens, y_tokens):\n",
    "    # Token-level LCS length (dynamic programming)\n",
    "    n, m = len(x_tokens), len(y_tokens)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(1, n+1):\n",
    "        xi = x_tokens[i-1]\n",
    "        row = dp[i]\n",
    "        prev_row = dp[i-1]\n",
    "        for j in range(1, m+1):\n",
    "            if xi == y_tokens[j-1]:\n",
    "                row[j] = prev_row[j-1] + 1\n",
    "            else:\n",
    "                row[j] = max(prev_row[j], row[j-1])\n",
    "    return dp[n][m]\n",
    "\n",
    "def _rouge_l_f1(pred, ref):\n",
    "    # Simple token-level ROUGE-L F1\n",
    "    ptoks = _normalize_text(pred).split()\n",
    "    rtoks = _normalize_text(ref).split()\n",
    "    if not ptoks or not rtoks:\n",
    "        return 0.0\n",
    "    lcs = _lcs_len(ptoks, rtoks)\n",
    "    prec = lcs / max(1, len(ptoks))\n",
    "    rec  = lcs / max(1, len(rtoks))\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "def _bag_of_words_f1(pred, ref):\n",
    "    from collections import Counter\n",
    "    ptoks = _normalize_text(pred).split()\n",
    "    rtoks = _normalize_text(ref).split()\n",
    "    pc, rc = Counter(ptoks), Counter(rtoks)\n",
    "    overlap = sum((pc & rc).values())\n",
    "    prec = overlap / max(1, sum(pc.values()))\n",
    "    rec  = overlap / max(1, sum(rc.values()))\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "pred_texts = []\n",
    "ref_texts  = []\n",
    "input_texts = []\n",
    "\n",
    "max_gen_tokens = MAX_TARGET_LENGTH\n",
    "gen_kwargs = dict(max_new_tokens=max_gen_tokens)\n",
    "\n",
    "print(\"Generating predictions on test split...\")\n",
    "for idx in tqdm(range(len(test_dataset))):\n",
    "    inp = test_dataset[idx][\"input_text\"]\n",
    "    ref = test_dataset[idx][\"target_text\"]\n",
    "    enc = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**enc, **gen_kwargs)\n",
    "    pred = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    \n",
    "    input_texts.append(inp)\n",
    "    pred_texts.append(pred)\n",
    "    ref_texts.append(ref)\n",
    "\n",
    "# Compute metrics\n",
    "def _exact_match(pred, ref):\n",
    "    return int(_normalize_text(pred) == _normalize_text(ref))\n",
    "\n",
    "ems = [ _exact_match(p, r) for p, r in zip(pred_texts, ref_texts) ]\n",
    "rouges = [ _rouge_l_f1(p, r) for p, r in zip(pred_texts, ref_texts) ]\n",
    "bows = [ _bag_of_words_f1(p, r) for p, r in zip(pred_texts, ref_texts) ]\n",
    "\n",
    "metrics = {\n",
    "    \"test_size\": len(test_dataset),\n",
    "    \"exact_match\": float(np.mean(ems)),\n",
    "    \"rougeL_f1\": float(np.mean(rouges)),\n",
    "    \"bow_f1\": float(np.mean(bows)),\n",
    "    \"avg_pred_len\": float(np.mean([len(_normalize_text(p).split()) for p in pred_texts])),\n",
    "    \"avg_ref_len\": float(np.mean([len(_normalize_text(r).split()) for r in ref_texts])),\n",
    "}\n",
    "\n",
    "print(\"\\n--- Test Metrics ---\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "# Show a few qualitative examples\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "for i in range(min(5, len(test_dataset))):\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(\"INPUT:\", input_texts[i][:300].replace(\"\\n\", \" \"))\n",
    "    print(\"REF:  \", ref_texts[i])\n",
    "    print(\"PRED: \", pred_texts[i])\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:40:09.551662Z",
     "iopub.status.busy": "2025-09-16T14:40:09.551409Z",
     "iopub.status.idle": "2025-09-16T14:40:30.375319Z",
     "shell.execute_reply": "2025-09-16T14:40:30.374472Z",
     "shell.execute_reply.started": "2025-09-16T14:40:09.551636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 1.6 MB/s eta 0:00:00\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 941.6 kB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 101.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 563.4/563.4 kB 27.1 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.8/485.8 kB 19.8 MB/s eta 0:00:00\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 74.3 MB/s eta 0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249e74cf4cd04adc864abf79b728d230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42585e68083942dba5b611e2dc5b07b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007799143d324ac29add07740d8b2802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ccb0dad48b41d693f627ddc4f355b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44679b29fd6495b8e1b1abdec251cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore skipped: cannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88957262c5ba4870b5b0f41f9fecfb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f8ce0d912844d19d536ead65ef095b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aa7d091f854c5a84c0f81eda1602a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3044f4d39d924277bf2639fb27e43c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625f96999ca34e63918fa228bfcd2f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-BERT skipped: cannot import name 'layer_type_validation' from 'transformers.configuration_utils' (/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py)\n",
      "\n",
      "--- Contextual Metrics ---\n",
      "tfidf_cosine_mean: 0.3406\n",
      "tfidf_cosine_median: 0.3311\n"
     ]
    }
   ],
   "source": [
    "# --- Contextual Metrics (append-only) ---\n",
    "# If your environment allows installs, keep these; otherwise the code will gracefully skip.\n",
    "try:\n",
    "    # quiet installs; remove -q if you want logs\n",
    "    import sys, subprocess\n",
    "    def _pip(pkg): \n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    _pip(\"bert-score\")\n",
    "    _pip(\"sentence-transformers\")\n",
    "except Exception as _e:\n",
    "    print(\"Package install skipped or failed:\", _e)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ctx_metrics = {}\n",
    "\n",
    "# 1) BERTScore (semantic precision/recall/F1) — robust for paraphrases\n",
    "try:\n",
    "    from bert_score import score as bertscore\n",
    "    P, R, F1 = bertscore(pred_texts, ref_texts, lang='en', rescale_with_baseline=True)\n",
    "    ctx_metrics['bertscore_precision'] = float(P.mean())\n",
    "    ctx_metrics['bertscore_recall']    = float(R.mean())\n",
    "    ctx_metrics['bertscore_f1']        = float(F1.mean())\n",
    "except Exception as e:\n",
    "    print(\"BERTScore skipped:\", e)\n",
    "\n",
    "# 2) Sentence-BERT cosine similarity — fast, strong baseline for contextual match\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    st_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device.type)\n",
    "    emb_pred = st_model.encode(pred_texts, convert_to_tensor=True, normalize_embeddings=True, batch_size=64, show_progress_bar=False)\n",
    "    emb_ref  = st_model.encode(ref_texts,  convert_to_tensor=True, normalize_embeddings=True, batch_size=64, show_progress_bar=False)\n",
    "    # cosine for aligned pairs (diagonal)\n",
    "    cos = util.cos_sim(emb_pred, emb_ref).diagonal().detach().cpu().numpy()\n",
    "    ctx_metrics['sbert_cosine_mean']   = float(np.mean(cos))\n",
    "    ctx_metrics['sbert_cosine_median'] = float(np.median(cos))\n",
    "except Exception as e:\n",
    "    print(\"Sentence-BERT skipped:\", e)\n",
    "    # 3) Fallback: TF-IDF cosine (still contextual-ish, but weaker than embeddings)\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "        X = tfidf.fit_transform([*pred_texts, *ref_texts])\n",
    "        n = len(pred_texts)\n",
    "        A, B = X[:n], X[n:]\n",
    "        sims = [cosine_similarity(A[i], B[i])[0, 0] for i in range(n)]\n",
    "        ctx_metrics['tfidf_cosine_mean']   = float(np.mean(sims))\n",
    "        ctx_metrics['tfidf_cosine_median'] = float(np.median(sims))\n",
    "    except Exception as ee:\n",
    "        print(\"TF-IDF fallback skipped:\", ee)\n",
    "\n",
    "print(\"\\n--- Contextual Metrics ---\")\n",
    "for k, v in ctx_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:40:30.376613Z",
     "iopub.status.busy": "2025-09-16T14:40:30.376362Z",
     "iopub.status.idle": "2025-09-16T14:40:30.583702Z",
     "shell.execute_reply": "2025-09-16T14:40:30.583037Z",
     "shell.execute_reply.started": "2025-09-16T14:40:30.376595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving FLAN-T5 LoRA adapter...\n",
      "Verifying save integrity...\n",
      "Found adapter_config.json: 767 bytes\n",
      "Found adapter_model.safetensors: 7,098,016 bytes\n",
      "Base model recorded in adapter: google/flan-t5-base\n",
      "Model saved to: /kaggle/working/flan-t5-action-extractor-final\n"
     ]
    }
   ],
   "source": [
    "# Save model with comprehensive verification\n",
    "FINAL_OUTPUT_DIR = \"/kaggle/working/flan-t5-action-extractor-final\"\n",
    "\n",
    "def save_and_verify_model():\n",
    "    \"\"\"Save model with thorough verification\"\"\"\n",
    "    try:\n",
    "        print(\"Saving FLAN-T5 LoRA adapter...\")\n",
    "        model.save_pretrained(FINAL_OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(FINAL_OUTPUT_DIR)\n",
    "        \n",
    "        # Verification steps\n",
    "        print(\"Verifying save integrity...\")\n",
    "        \n",
    "        # Check files exist\n",
    "        required_files = ['adapter_config.json', 'adapter_model.safetensors']\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(FINAL_OUTPUT_DIR, file)\n",
    "            if os.path.exists(file_path):\n",
    "                size = os.path.getsize(file_path)\n",
    "                print(f\"Found {file}: {size:,} bytes\")\n",
    "            else:\n",
    "                print(f\"Missing {file}\")\n",
    "        \n",
    "        # Attempt to load PEFT config if present\n",
    "        try:\n",
    "            peft_config_path = os.path.join(FINAL_OUTPUT_DIR, \"adapter_config.json\")\n",
    "            if os.path.exists(peft_config_path):\n",
    "                with open(peft_config_path, \"r\") as f:\n",
    "                    peft_cfg = json.load(f)\n",
    "                base_model_path = peft_cfg.get(\"base_model_name_or_path\", None)\n",
    "                if base_model_path:\n",
    "                    print(f\"Base model recorded in adapter: {base_model_path}\")\n",
    "                else:\n",
    "                    print(\"Warning: base_model_name_or_path not found in adapter_config.json\")\n",
    "            else:\n",
    "                print(\"No adapter_config.json found; skipping PEFT config check.\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"PEFT verification failed: {e}\")\n",
    "            \n",
    "        print(f\"Model saved to: {FINAL_OUTPUT_DIR}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Save error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save model\n",
    "save_success = save_and_verify_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMETER SET-2 (PERFORMED WORSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:40:30.584784Z",
     "iopub.status.busy": "2025-09-16T14:40:30.584512Z",
     "iopub.status.idle": "2025-09-16T14:45:02.633365Z",
     "shell.execute_reply": "2025-09-16T14:45:02.632608Z",
     "shell.execute_reply.started": "2025-09-16T14:40:30.584754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.52.4\n",
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "Loading dataset from: /kaggle/input/cleaned-json/prepared_data_cleaned.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c6c6056cde4b00b031626e89d39b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 300 rows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07f5968eaad42698e38de27490c04d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 240 | Test: 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d8eea404de4d4599045285ac683a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9e39f3f6f4cec848c41c41dd4a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,620,288 || all params: 252,198,144 || trainable%: 1.8320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 02:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Rougel F1</th>\n",
       "      <th>Bow F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>0.497748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433814</td>\n",
       "      <td>0.449217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done.\n",
      "Evaluating on test set…\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.4811\n",
      "test_exact_match: 0.0000\n",
      "test_rougeL_f1: 0.2995\n",
      "test_bow_f1: 0.3027\n",
      "test_runtime: 103.9697\n",
      "test_samples_per_second: 0.5770\n",
      "test_steps_per_second: 0.0770\n",
      "\n",
      "--- Sample Predictions ---\n",
      "REF : [\"due_date\": \"None\", \"owner\": \"Olivia\", \"task\": \"Email Shepperd and answer his questions about Amanda's offer\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Call Hoffmann\"]\n",
      "PRED: [\"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"None\", \"owner\": \"Emily\", \"task\": \"Email Shepperd\", \"due_date\": \"\n",
      "--------------------------------------------------------------------------------\n",
      "REF : [\"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Find work in NY\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Move outside Europe\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Find work in an Asian country (e.g. Indonesia)\", \"due_date\": \"Summer\", \"owner\": \"Victoria\", \"task\": \"Plan a trip to Greece\", \"due_date\": \"August\", \"owner\": \"William\", \"task\": \"Plan a trip to one of the islands with a beach (e.g. Cuba)\"]\n",
      "PRED: [\"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"None\", \"owner\": \"Victoria\", \"task\": \"Get ready for Greece\", \"due_date\": \"\n",
      "--------------------------------------------------------------------------------\n",
      "REF : [\"due_date\": \"Friday night\", \"owner\": \"Joanna and Daniel\", \"task\": \"Attend Mike's birthday party\", \"due_date\": \"Friday night\", \"owner\": \"Joanna and Daniel\", \"task\": \"Attend Jimmy's party (optional)\", \"due_date\": \"Saturday night\", \"owner\": \"Joanna and Daniel\", \"task\": \"Go grocery shopping\", \"due_date\": \"Saturday night\", \"owner\": \"Joanna and Daniel\", \"task\": \"Buy cleaning products\", \"due_date\": \"Saturday night\", \"owner\": \"Joanna and Daniel\", \"task\": \"Clean the apartment\", \"due_date\": \"This Saturday\", \"owner\": \"Joanna and Daniel\", \"task\": \"Have late lunch with Daniel's parents\", \"due_date\": \"After Saturday night\",\n",
      "PRED: [\"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Enumerate everything we've got going on this weekend\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the apartment\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the apartment\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the apartment\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the apartment\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the apartment\", \"due_date\": \"None\", \"owner\": \"Joanna\", \"task\": \"Clean up the\n",
      "--------------------------------------------------------------------------------\n",
      "REF : [\"due_date\": \"None\", \"owner\": \"Johana\", \"task\": \"Confirm dinner plans for tomorrow at 8:30 pm\", \"due_date\": \"2023-09-21\", \"owner\": \"Johana\", \"task\": \"Confirm dinner plans for this Friday\", \"due_date\": \"None\", \"owner\": \"Ben\", \"task\": \"Report marathon run on Sunday\"]\n",
      "PRED: [\"due_date\": \"None\", \"owner\": \"Johana\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\", \"owner\": \"Yoli\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\", \"owner\": \"Yoli\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\", \"owner\": \"Yoli\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\", \"owner\": \"Yoli\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\", \"owner\": \"Yoli\", \"task\": \"Confirm for pre-election diner at home\", \"due_date\": \"None\",\n",
      "--------------------------------------------------------------------------------\n",
      "REF : [\"due_date\": \"None\", \"owner\": \"Ronald\", \"task\": \"Apologize to the boss\"]\n",
      "PRED: [\"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Apologize boss\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\", \"due_date\": \"None\", \"owner\": \"Ron\", \"task\": \"Explain regret\n",
      "--------------------------------------------------------------------------------\n",
      "Saved LoRA adapter to: /kaggle/working/flan_t5_action_json/lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# FLAN-T5 JSON action-item finetune (HF 4.52-safe)\n",
    "# - Canonical JSON targets\n",
    "# - Robust LoRA coverage (q,k,v,o,wi,wo)\n",
    "# - SAFE decode (fixes OverflowError)\n",
    "# - Version-adaptive TrainingArguments AND Trainer init (no label_names if unsupported)\n",
    "\n",
    "import os, json, warnings, inspect\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# -----------------------------\n",
    "# Env & reproducibility\n",
    "# -----------------------------\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name())\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "MODEL_NAME      = \"google/flan-t5-base\"\n",
    "DATASET_PATH    = \"/kaggle/input/cleaned-json/prepared_data_cleaned.jsonl\"\n",
    "OUTPUT_DIR      = \"/kaggle/working/flan_t5_action_json\"\n",
    "MAX_INPUT_LEN   = 512\n",
    "MAX_TARGET_LEN  = 256\n",
    "\n",
    "# -----------------------------\n",
    "# Data loading & canonicalization\n",
    "# -----------------------------\n",
    "def _canon_json(obj: Any) -> str:\n",
    "    return json.dumps(obj, ensure_ascii=False, sort_keys=True)\n",
    "\n",
    "def load_action_item_dataset(path: str) -> Dataset:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {path}\")\n",
    "    print(f\"Loading dataset from: {path}\")\n",
    "    ds = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "    print(f\"Loaded {len(ds)} rows.\")\n",
    "    return ds\n",
    "\n",
    "raw = load_action_item_dataset(DATASET_PATH)\n",
    "\n",
    "def to_instruction_format(ex: Dict[str, Any]) -> Dict[str, str]:\n",
    "    tgt = ex.get(\"actions\", [])\n",
    "    if isinstance(tgt, str):\n",
    "        try:\n",
    "            tgt = _canon_json(json.loads(tgt))\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        tgt = _canon_json(tgt)\n",
    "    prompt = (\n",
    "        \"Extract ONLY action items as a JSON list. \"\n",
    "        \"Return JSON and nothing else. Use fields: task, owner, due_date.\\n\\n\"\n",
    "        f\"Dialog:\\n{ex['dialogue']}\"\n",
    "    )\n",
    "    return {\"input_text\": prompt, \"target_text\": tgt}\n",
    "\n",
    "ds = raw.map(to_instruction_format, remove_columns=raw.column_names)\n",
    "splits = ds.train_test_split(test_size=0.2, seed=SEED)\n",
    "train_ds, test_ds = splits[\"train\"], splits[\"test\"]\n",
    "print(f\"Train: {len(train_ds)} | Test: {len(test_ds)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer & preprocessing\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_batch(batch: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input_text\"],\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        truncation=True\n",
    "    )\n",
    "    try:\n",
    "        labels = tokenizer(\n",
    "            text_target=batch[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LEN,\n",
    "            truncation=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        # very old fallback\n",
    "        labels = tokenizer(\n",
    "            batch[\"target_text\"],\n",
    "            max_length=MAX_TARGET_LEN,\n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_batch, batched=True, remove_columns=train_ds.column_names)\n",
    "tokenized_test  = test_ds.map(preprocess_batch,  batched=True, remove_columns=test_ds.column_names)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MODEL_NAME)\n",
    "\n",
    "# -----------------------------\n",
    "# Model & LoRA\n",
    "# -----------------------------\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.config.use_cache = False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "if getattr(model.config, \"decoder_start_token_id\", None) is None:\n",
    "    model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    target_modules=[\"q\",\"k\",\"v\",\"o\",\"wi\",\"wo\"]  # T5 attention + FFN\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics (lightweight) + SAFE unwrap/sanitize\n",
    "# -----------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "def _lcs_len(x, y):\n",
    "    n, m = len(x), len(y)\n",
    "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
    "    for i in range(1, n+1):\n",
    "        xi = x[i-1]; row = dp[i]; prev = dp[i-1]\n",
    "        for j in range(1, m+1):\n",
    "            row[j] = prev[j-1] + 1 if xi == y[j-1] else max(prev[j], row[j-1])\n",
    "    return dp[n][m]\n",
    "\n",
    "def rougeL_f1(pred, ref):\n",
    "    pt, rt = _norm(pred).split(), _norm(ref).split()\n",
    "    if not pt or not rt: return 0.0\n",
    "    l = _lcs_len(pt, rt); p = l/max(1,len(pt)); r = l/max(1,len(rt))\n",
    "    return 0.0 if p+r==0 else 2*p*r/(p+r)\n",
    "\n",
    "def bow_f1(pred, ref):\n",
    "    from collections import Counter\n",
    "    pt, rt = _norm(pred).split(), _norm(ref).split()\n",
    "    pc, rc = Counter(pt), Counter(rt)\n",
    "    overlap = sum((pc & rc).values())\n",
    "    p = overlap / max(1,sum(pc.values()))\n",
    "    r = overlap / max(1,sum(rc.values()))\n",
    "    return 0.0 if p+r==0 else 2*p*r/(p+r)\n",
    "\n",
    "def _unwrap_sequences(preds):\n",
    "    # Handle tuples from Trainer (sequences, scores, …), tensors, lists\n",
    "    if isinstance(preds, (list, tuple)):\n",
    "        preds = preds[0]\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "    preds = np.asarray(preds)\n",
    "    if preds.dtype.kind != \"i\":\n",
    "        preds = preds.astype(np.int64, copy=False)\n",
    "    preds = np.where(preds < 0, tokenizer.pad_token_id, preds)\n",
    "    return preds\n",
    "\n",
    "def _unwrap_labels(labels):\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "    labels = np.asarray(labels)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    return labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = getattr(eval_pred, \"predictions\", None)\n",
    "    labels_ids  = getattr(eval_pred, \"label_ids\",  None)\n",
    "    if predictions is None:\n",
    "        predictions, labels_ids = eval_pred\n",
    "\n",
    "    preds_ids = _unwrap_sequences(predictions)\n",
    "    labels_ids = _unwrap_labels(labels_ids)\n",
    "\n",
    "    preds = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "    refs  = tokenizer.batch_decode(labels_ids,  skip_special_tokens=True)\n",
    "\n",
    "    em  = float(np.mean([_norm(p)==_norm(r) for p,r in zip(preds, refs)]))\n",
    "    rL  = float(np.mean([rougeL_f1(p,r) for p,r in zip(preds, refs)]))\n",
    "    bow = float(np.mean([bow_f1(p,r) for p,r in zip(preds, refs)]))\n",
    "    return {\"exact_match\": em, \"rougeL_f1\": rL, \"bow_f1\": bow}\n",
    "\n",
    "# -----------------------------\n",
    "# Version-adaptive TrainingArguments\n",
    "# -----------------------------\n",
    "def build_args(**kw):\n",
    "    sig = inspect.signature(Seq2SeqTrainingArguments.__init__)\n",
    "    allowed = set(sig.parameters.keys())\n",
    "    out = {k:v for k,v in kw.items() if k in allowed}\n",
    "\n",
    "    # eval strategy name across versions\n",
    "    if \"evaluation_strategy\" in allowed and \"evaluation_strategy\" in kw:\n",
    "        out[\"evaluation_strategy\"] = kw[\"evaluation_strategy\"]\n",
    "    elif \"eval_strategy\" in allowed and \"evaluation_strategy\" in kw:\n",
    "        out[\"eval_strategy\"] = kw[\"evaluation_strategy\"]\n",
    "\n",
    "    if \"predict_with_generate\" in kw and \"predict_with_generate\" not in allowed:\n",
    "        out.pop(\"predict_with_generate\", None)\n",
    "    if \"generation_max_length\" in kw and \"generation_max_length\" not in allowed:\n",
    "        out.pop(\"generation_max_length\", None)\n",
    "\n",
    "    return Seq2SeqTrainingArguments(**out)\n",
    "\n",
    "base_args = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    seed=SEED,Training done.\n",
    "Evaluating on test set…\n",
    "test_loss: 0.4811\n",
    "test_exact_match: 0.0000\n",
    "test_rougeL_f1: 0.2995\n",
    "test_bow_f1: 0.3027\n",
    "test_runtime: 103.9697\n",
    "test_samples_per_second: 0.5770\n",
    "test_steps_per_second: 0.0770\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL_f1\",\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=[],\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LEN,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported():\n",
    "        base_args[\"bf16\"] = True\n",
    "    else:\n",
    "        base_args[\"fp16\"] = True\n",
    "\n",
    "training_args = build_args(**base_args)\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer (version-adaptive kwargs: no label_names if unsupported)\n",
    "# -----------------------------\n",
    "trainer_kwargs = dict(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "sig_tr = inspect.signature(Seq2SeqTrainer.__init__)\n",
    "if \"label_names\" in sig_tr.parameters:\n",
    "    trainer_kwargs[\"label_names\"] = [\"labels\"]  # only if supported\n",
    "\n",
    "trainer = Seq2SeqTrainer(**trainer_kwargs)\n",
    "\n",
    "print(\"Starting training…\")\n",
    "trainer.train()\n",
    "print(\"Training done.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation / prediction\n",
    "# -----------------------------\n",
    "print(\"Evaluating on test set…\")\n",
    "predictions = trainer.predict(tokenized_test, max_length=MAX_TARGET_LEN, num_beams=4)\n",
    "\n",
    "# safe manual decode\n",
    "pred_ids = predictions.predictions[0] if isinstance(predictions.predictions, (list, tuple)) else predictions.predictions\n",
    "pred_ids = np.asarray(pred_ids)\n",
    "pred_ids = np.where(pred_ids < 0, tokenizer.pad_token_id, pred_ids)\n",
    "decoded_preds = tokenizer.batch_decode(pred_ids.astype(np.int64, copy=False), skip_special_tokens=True)\n",
    "decoded_refs  = tokenizer.batch_decode(\n",
    "    np.where(predictions.label_ids!=-100, predictions.label_ids, tokenizer.pad_token_id),\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "for k, v in predictions.metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, (int,float)) else f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "for i in range(min(5, len(decoded_preds))):\n",
    "    print(\"REF :\", decoded_refs[i])\n",
    "    print(\"PRED:\", decoded_preds[i])\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# -----------------------------\n",
    "# Save LoRA adapter\n",
    "# -----------------------------\n",
    "SAVE_DIR = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"Saved LoRA adapter to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:45:02.634442Z",
     "iopub.status.busy": "2025-09-16T14:45:02.634194Z",
     "iopub.status.idle": "2025-09-16T14:45:07.302737Z",
     "shell.execute_reply": "2025-09-16T14:45:07.301838Z",
     "shell.execute_reply.started": "2025-09-16T14:45:02.634423Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/flan_t5_action_json/ (stored 0%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/ (stored 0%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/tokenizer_config.json (deflated 95%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/tokenizer.json (deflated 74%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/README.md (deflated 66%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/adapter_config.json (deflated 55%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/spiece.model (deflated 48%)\n",
      "  adding: kaggle/working/flan_t5_action_json/lora_adapter/special_tokens_map.json (deflated 85%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/ (stored 0%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/adapter_model.safetensors (deflated 8%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/tokenizer_config.json (deflated 95%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/trainer_state.json (deflated 58%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/tokenizer.json (deflated 74%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/README.md (deflated 66%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/training_args.bin (deflated 51%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/adapter_config.json (deflated 55%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/spiece.model (deflated 48%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/optimizer.pt (deflated 8%)\n",
      "  adding: kaggle/working/flan_t5_action_json/checkpoint-75/special_tokens_map.json (deflated 85%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/flan_t5_action_json.zip /kaggle/working/flan_t5_action_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8260897,
     "sourceId": 13045698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8283078,
     "sourceId": 13078327,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
